{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate p-values and networks for a large-scale benchmark study for \n",
    "### subnetwork discovery\n",
    "## 1. Decide target subnetworks from database\n",
    "## 2. load PPIs\n",
    "## 3. Generate p-values and to q score, lfdr, gene lists...\n",
    "## 4. send data to computing clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing\n",
    "import pandas as pd\n",
    "import mygene\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from scipy.stats import beta, uniform, norm\n",
    "from locfdr_python.locfdr_for_plot import locfdr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from collections import OrderedDict\n",
    "import subprocess\n",
    "mg = mygene.MyGeneInfo()\n",
    "## loading hugo names\n",
    "hugo = pd.read_csv('/Users/leyang/Dropbox/Projects/Subnetwork_survey/code/Database/hugo_symbol.txt')\n",
    "hugo = list(hugo['Approved symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed66ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "# writing networkx graph into file\n",
    "def write_nx_to_file(G,index_file,edge_file,sif_file,tsv_file):\n",
    "    gene_list = list(G.nodes())\n",
    "    edge_list = list(G.edges())\n",
    "    gene_list.sort()\n",
    "    index_gene = dict(zip(gene_list,range(1,len(gene_list)+1)))\n",
    "    edge_list_num = [(index_gene[edge[0]],index_gene[edge[1]]) for edge in edge_list]\n",
    "    with open(index_file,'w') as f:\n",
    "        for k in index_gene.keys():\n",
    "            f.write('{}\\t{}\\n'.format(index_gene[k],k))\n",
    "    with open(edge_file,'w') as f:\n",
    "        for i in range(len(edge_list_num)):\n",
    "            f.write('{}\\t{}\\t{}\\n'.format(edge_list_num[i][0],edge_list_num[i][1],1))\n",
    "    with open(sif_file,'w') as f:\n",
    "        f.write('ID_interactor_A\\tppi\\tID_interactor_B\\n')\n",
    "        for i in range(len(edge_list)):\n",
    "            f.write('{}\\t{}\\t{}\\n'.format(edge_list[i][0],'ppi',edge_list[i][1]))\n",
    "    with open(tsv_file,'w') as f:\n",
    "        f.write('ID_interactor_A\\tID_interactor_B\\n')\n",
    "        for i in range(len(edge_list)):\n",
    "            f.write('{}\\t{}\\n'.format(edge_list[i][0],edge_list[i][1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1: Decide target subnetworks\n",
    "## (a) load databases: CORUM and Reactome\n",
    "corum_raw = pd.read_csv('/Users/leyang/Dropbox/Projects/Subnetwork_survey/code/Database/coreComplexes.txt',delimiter='\\t')\n",
    "reactome_raw = pd.read_csv('/Users/leyang/Dropbox/Projects/Subnetwork_survey/code/Database/human_pathway_lowlevel_ncbi.txt','\\t')\n",
    "\n",
    "## (b) pre-preocessing the complexes\n",
    "corum_human = corum_raw[corum_raw['Organism'] == 'Human']\n",
    "complex_ID = list(corum_human['ComplexID'])\n",
    "complex_sub_entrez = list(corum_human['subunits(Entrez IDs)'])\n",
    "complex_name = list(corum_human['ComplexName'])\n",
    "complex_sub_gene_name = list(corum_human['subunits(Gene name)'])\n",
    "complex_sub_entrez_split = [x.split(';') for x in complex_sub_entrez]\n",
    "complex_sub_gene_name_split = [x.split(';') for x in complex_sub_gene_name]\n",
    "complex_sub_size = [len(x) for x in complex_sub_entrez_split]\n",
    "large_index = [k for k in range(len(complex_sub_size)) if complex_sub_size[k] >= 10]\n",
    "\n",
    "complex_ID_large = [complex_ID[x] for x in large_index]\n",
    "complex_name_large = [complex_name[x] for x in large_index]\n",
    "complex_sub_entrez_split_large = [complex_sub_entrez_split[x] for x in large_index]\n",
    "complex_sub_gene_name_split_large = [complex_sub_gene_name_split[x] for x in large_index]\n",
    "complex_sub_size_large = [complex_sub_size[x] for x in large_index]\n",
    "\n",
    "corum_tuple = zip(complex_ID_large, complex_name_large,complex_sub_size_large, complex_sub_entrez_split_large,complex_sub_gene_name_split_large)\n",
    "\n",
    "## standarize gene names according to hugo symbol and entrez id\n",
    "corum_standarize = []\n",
    "for (i,name,s,entrez,gene) in corum_tuple:\n",
    "    gene_list = []\n",
    "    for (n,e) in zip(gene,entrez):\n",
    "        g_tmp = 'None'\n",
    "        if n not in hugo:\n",
    "            out = mg.query(e, fields='symbol',species='human')\n",
    "            if out['hits']:  \n",
    "                symbol = out['hits'][0]['symbol']\n",
    "                if symbol in hugo:\n",
    "                    g_tmp = symbol\n",
    "        else:\n",
    "            g_tmp = n\n",
    "        gene_list.append(g_tmp)\n",
    "    corum_standarize.append((i,name,s,gene_list))\n",
    "\n",
    "corum_standarize = [x for x in corum_standarize if 'None' not in x[3]]\n",
    "\n",
    "remove_index = []\n",
    "for i in range(len(corum_standarize)):\n",
    "    for j in range(len(corum_standarize)):\n",
    "        if i != j:\n",
    "            intersect_length = len([val for val in corum_standarize[i][3] if val in corum_standarize[j][3]])\n",
    "            if intersect_length / len(corum_standarize[i][3]) > 0.8:\n",
    "                remove_index.append(i)\n",
    "\n",
    "corum_final = [corum_standarize[i] for i in range(len(corum_standarize)) if i not in remove_index]\n",
    "corum_final_save = [(i,name,s,','.join(gene_list)) for (i,name,s,gene_list) in corum_final]\n",
    "\n",
    "## (c) pre-preocessing the reactome pathways\n",
    "pathway_name = list(set(reactome_raw['pathway_name']))\n",
    "reactome_standarize = []\n",
    "for p in pathway_name:\n",
    "    pp = reactome_raw[reactome_raw['pathway_name'] == p]\n",
    "    gene_raw = list(pp['Gene'])\n",
    "    gene_list = [gene_raw[i] for i in range(len(gene_raw)) if gene_raw[i] in hugo]\n",
    "    evidence_raw = list(pp['evidence'])\n",
    "    evidence_list = [evidence_raw[i] for i in range(len(gene_raw)) if gene_raw[i] in hugo]\n",
    "    reactome_standarize.append((p,len(gene_list),gene_list,evidence_list))\n",
    "\n",
    "reactome_standarize = [x for x in reactome_standarize if x[1] >= 10]\n",
    "\n",
    "remove_index = []\n",
    "for i in range(len(reactome_standarize)):\n",
    "    for j in range(len(reactome_standarize)):\n",
    "        if i != j:\n",
    "            intersect_length = len([val for val in reactome_standarize[i][2] if val in reactome_standarize[j][2]])\n",
    "            if intersect_length / len(reactome_standarize[i][3]) > 0.8:\n",
    "                remove_index.append(i)\n",
    "                \n",
    "reactome_final = [reactome_standarize[i] for i in range(len(reactome_standarize)) if i not in set(remove_index)]\n",
    "reactome_final_save = [(name,s,','.join(gene_list),','.join(evidence_list)) for (name,s,gene_list,evidence_list) in reactome_final]\n",
    "\n",
    "df_corum = pd.DataFrame(corum_final_save,columns=['ID','Name','Size','Genes'])\n",
    "df_corum.to_csv('./Database/corum.txt',sep='\\t')\n",
    "df_reactome = pd.DataFrame(reactome_final_save,columns=['Name','Size','Genes','Evidence'])\n",
    "df_reactome.to_csv('./Database/reactome.txt',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdf203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Part 2: processing PPIs\n",
    "# BIOGRID network: https://downloads.thebiogrid.org/File/BioGRID/Release-Archive/BIOGRID-4.4.212/BIOGRID-ALL-4.4.212.tab3.zip\n",
    "# load raw file\n",
    "ppi_biogrid = pd.read_table('./network_raw/BIOGRID-ALL-4.4.212.tab3.txt',header=0,sep='\\t')\n",
    "\n",
    "# extract homp sapien interactions: only the entry with both A and B are homo sapiens are extracted\n",
    "tmp1 = ppi_biogrid['Organism Name Interactor A'] == 'Homo sapiens'\n",
    "tmp2 = ppi_biogrid['Organism Name Interactor B'] == 'Homo sapiens'\n",
    "tmp_index1 = [i for i in range(len(tmp1)) if tmp1[i]]\n",
    "tmp_index2 = [i for i in range(len(tmp2)) if tmp2[i]]\n",
    "homo_index = set(set(tmp_index1) & set(tmp_index2))\n",
    "\n",
    "# entrez id of start and end\n",
    "entrez_start = [ppi_biogrid['Entrez Gene Interactor A'][i] for i in homo_index]\n",
    "entrez_end = [ppi_biogrid['Entrez Gene Interactor B'][i] for i in homo_index]\n",
    "\n",
    "# build a ID converter using mygene: from entrez to offical symbols\n",
    "IDs = set(entrez_start + entrez_end)\n",
    "mg = mygene.MyGeneInfo()\n",
    "# query all genes with the same IDtype from mygene, 14902 input query terms found dup hits (two hits are the same)\n",
    "# 101 no hits, sample some and find them removed from NCBI\n",
    "query_result = mg.querymany(IDs, scopes='entrezgene',fields='symbol',species='human',returnall=True) \n",
    "idconverter = dict([(x['query'],x['symbol']) for x in query_result['out'] if 'symbol' in x.keys()])\n",
    "\n",
    "# manually annotated\n",
    "idconverter['4549'] = 'MT-RNR1'\n",
    "idconverter.pop('7795')\n",
    "\n",
    "## build graph\n",
    "biogrid = nx.Graph()\n",
    "for (start,end) in zip(entrez_start,entrez_end):\n",
    "    start = str(start)\n",
    "    end = str(end)\n",
    "    if start in idconverter.keys() and end in idconverter.keys():\n",
    "        geneA = idconverter[start]\n",
    "        geneB = idconverter[end]\n",
    "        if geneA in approved_gene_list and geneB in approved_gene_list:\n",
    "            biogrid.add_edge(geneA,geneB)\n",
    "biogrid.remove_edges_from(list(nx.selfloop_edges(biogrid)))\n",
    "\n",
    "print(\"BioGRID 4.4.212\")\n",
    "print(\"Number of Nodes:\",len(biogrid.nodes()))\n",
    "print(\"Number of Edges:\",len(biogrid.edges()))\n",
    "print(\"Degree of TP53:\",biogrid.degree('TP53'))\n",
    "print(\"Degree of PIK3CA:\",biogrid.degree('PIK3CA'))\n",
    "print(\"Degree of CDH1:\",biogrid.degree('CDH1'))\n",
    "## write index file, edge_list for FDRnet and multiplexFDRnet and sif and tsv for other methods\n",
    "write_nx_to_file(biogrid,\"biogrid_index_gene\",\"./networks/biogrid_edge_list\",\"./networks/biogrid_edge_list.sif\",\"./networks/biogrid_edge_list.tsv\")\n",
    "\n",
    "# iRefIndex18 network: https://irefindex.vib.be/download/irefindex/data/archive/release_18.0/psi_mitab/MITAB2.6/\n",
    "ppi_irefindex18 = pd.read_table('./network_raw/9606.mitab.06-11-2021.txt',header=0,sep='\\t')\n",
    "tmp = [x.startswith('complex:') for x in ppi_irefindex18['#uidA']]\n",
    "index = [i for i in range(len(tmp)) if not tmp[i]] # remove complex entries\n",
    "\n",
    "alias_start = [ppi_irefindex18['altA'][i].split('|')[0] for i in index] # get first alt\n",
    "alias_end = [ppi_irefindex18['altB'][i].split('|')[0] for i in index]\n",
    "\n",
    "# using entrez, refseq and uniprot id to get symbols\n",
    "entrez_items = set([x.split(':')[1].strip() for x in alias_start if x.startswith('entrezgene/locuslink:')] + [x.split(':')[1].strip() for x in alias_end if x.startswith('entrezgene/locuslink:')]) #2158777\n",
    "refseq_items = set([x.split(':')[1].strip() for x in alias_start if x.startswith('refseq:')] + [x.split(':')[1].strip() for x in alias_end if x.startswith('refseq:')]) # 173689\n",
    "uniprot_items = set([x.split(':')[1].strip() for x in alias_start if x.startswith('uniprotkb:')] + [x.split(':')[1].strip() for x in alias_end if x.startswith('uniprotkb:')]) #90374\n",
    "\n",
    "\n",
    "# build idconverters for entrez, uniprot and refseq\n",
    "mg = mygene.MyGeneInfo()\n",
    "query_entrez = mg.querymany(entrez_items, scopes='entrezgene',fields='symbol',species='human',returnall=True)\n",
    "query_uniprot = mg.querymany(uniprot_items, scopes='uniprot',fields='symbol',species='human',returnall=True)\n",
    "query_refseq = mg.querymany(refseq_items, scopes='refseq',fields='symbol',species='human',returnall=True)\n",
    "idconverter_entrez = dict([(x['query'],x['symbol']) for x in query_entrez['out'] if 'symbol' in x.keys()])\n",
    "idconverter_uniprot = dict([(x['query'],x['symbol']) for x in query_uniprot['out'] if 'symbol' in x.keys()])\n",
    "idconverter_refseq = dict([(x['query'],x['symbol']) for x in query_refseq['out'] if 'symbol' in x.keys()])\n",
    "\n",
    "# build graph\n",
    "irefindex18 = nx.Graph()\n",
    "for (start,end) in zip(alias_start,alias_end):\n",
    "    #check which type the gene is annotated, get the id\n",
    "    # geneA\n",
    "    if start.startswith('entrez'):\n",
    "        start = start[21:]\n",
    "        if start in idconverter_entrez.keys():\n",
    "            geneA = idconverter_entrez[start]\n",
    "    elif start.startswith('uniprotkb:'):\n",
    "        start = start[10:]\n",
    "        if start in idconverter_uniprot.keys():\n",
    "            geneA = idconverter_uniprot[start]\n",
    "    elif start.startswith('refseq:'):\n",
    "        start = start[7:]\n",
    "        if start in idconverter_refseq.keys():\n",
    "            geneA = idconverter_refseq[start]\n",
    "    else:\n",
    "        geneA = []\n",
    "        \n",
    "    # geneB\n",
    "    if end.startswith('entrez'):\n",
    "        end = end[21:]\n",
    "        if end in idconverter_entrez.keys():\n",
    "            geneB = idconverter_entrez[end]\n",
    "    elif end.startswith('uniprotkb:'):\n",
    "        end = end[10:]\n",
    "        if end in idconverter_uniprot.keys():\n",
    "            geneB = idconverter_uniprot[end]\n",
    "    elif end.startswith('refseq:'):\n",
    "        end = end[7:]\n",
    "        if end in idconverter_refseq.keys():\n",
    "            geneB = idconverter_refseq[end]\n",
    "    else:\n",
    "        geneB = []\n",
    "    \n",
    "    if geneA in approved_gene_list and geneB in approved_gene_list:\n",
    "        irefindex18.add_edge(geneA,geneB)\n",
    "irefindex18.remove_edges_from(list(nx.selfloop_edges(irefindex18)))\n",
    "\n",
    "print(\"iRefindex 18.0\")\n",
    "print(\"Number of Nodes:\",len(irefindex18.nodes()))\n",
    "print(\"Number of Edges:\",len(irefindex18.edges()))\n",
    "print(\"Degree of TP53:\",irefindex18.degree('TP53'))\n",
    "print(\"Degree of PIK3CA:\",irefindex18.degree('PIK3CA'))\n",
    "print(\"Degree of CDH1:\",irefindex18.degree('CDH1'))\n",
    "write_nx_to_file(irefindex18,\"irefindex18_index_gene\",\"./networks/irefindex18_edge_list\",\"./networks/irefindex18_edge_list.sif\",\"./networks/irefindex18_edge_list.tsv\")\n",
    "\n",
    "# ReactomeFI 2021: https://reactome.org/download-data\n",
    "ppi_reactome2021 = pd.read_table('./network_raw/FIsInGene_122921_with_annotations.txt',header=0,sep='\\t')\n",
    "start_gene = ppi_reactome2021['Gene1']\n",
    "end_gene = ppi_reactome2021['Gene2']\n",
    "reactome21 = nx.Graph()\n",
    "for (start,end) in zip(start_gene,end_gene):\n",
    "    if start in approved_gene_list and end in approved_gene_list:\n",
    "        reactome21.add_edge(start,end)\n",
    "        \n",
    "reactome21.remove_edges_from(list(nx.selfloop_edges(reactome21)))\n",
    "print(\"ReactomeFI 2021\")\n",
    "print(\"Number of Nodes:\",len(reactome21.nodes()))\n",
    "print(\"Number of Edges:\",len(reactome21.edges()))\n",
    "print(\"Degree of TP53:\",reactome21.degree('TP53'))\n",
    "print(\"Degree of PIK3CA:\",reactome21.degree('PIK3CA'))\n",
    "print(\"Degree of CDH1:\",reactome21.degree('CDH1'))\n",
    "write_nx_to_file(reactome21,\"reactome21_index_gene\",\"./networks/reactome21_edge_list\",\"./networks/reactome21_edge_list.sif\",\"./networks/reactome21_edge_list.tsv\")\n",
    "\n",
    "# STRING v11.0:\n",
    "ppi_string = pd.read_table('./network_raw/9606.protein.links.v11.5.txt',header=0,sep=' ')\n",
    "ppi_string_annotation = pd.read_table('./network_raw/9606.protein.info.v11.5.txt',header=0,sep='\\t')\n",
    "high_score = ppi_string['combined_score'] >= 900\n",
    "high_index = [i for i in range(len(high_score)) if high_score[i]]\n",
    "\n",
    "# id converted from string database\n",
    "ppi_string_annotation = pd.read_table('./network_raw/9606.protein.info.v11.5.txt',header=0,sep='\\t')\n",
    "idconverter_string = dict(zip(ppi_string_annotation['#string_protein_id'],ppi_string_annotation['preferred_name'])) \n",
    "\n",
    "\n",
    "# get high confidence ensemble ids\n",
    "start_ensembl = [ppi_string['protein1'][i] for i in high_index]\n",
    "end_ensembl = [ppi_string['protein2'][i] for i in high_index]\n",
    "\n",
    "string_900 = nx.Graph()\n",
    "for (start,end) in zip(start_ensembl,end_ensembl):\n",
    "    if start in idconverter_string.keys() and end in idconverter_string.keys():\n",
    "        geneA = idconverter_string[start]\n",
    "        geneB = idconverter_string[end]\n",
    "        if geneA in approved_gene_list and geneB in approved_gene_list:\n",
    "            string_900.add_edge(geneA,geneB)\n",
    "\n",
    "string_900.remove_edges_from(list(nx.selfloop_edges(string_900)))\n",
    "print(\"STRING v11 high confidence (900)\")\n",
    "print(\"Number of Nodes:\",len(string_900.nodes()))\n",
    "print(\"Number of Edges:\",len(string_900.edges()))\n",
    "print(\"Degree of TP53:\",string_900.degree('TP53'))\n",
    "print(\"Degree of PIK3CA:\",string_900.degree('PIK3CA'))\n",
    "print(\"Degree of CDH1:\",string_900.degree('CDH1'))\n",
    "write_nx_to_file(string_900,\"string_index_gene\",\"./networks/string_edge_list\",\"./networks/string_edge_list.sif\",\"./networks/string_edge_list.tsv\")\n",
    "\n",
    "# merge biogrid, irefindex18, reactome21, string_900\n",
    "merge_bioirefreastr = nx.compose(nx.compose(nx.compose(biogrid,irefindex18),reactome21),string_900)\n",
    "\n",
    "print(\"merge_bioirefreastr\")\n",
    "print(\"Number of Nodes:\",len(merge_bioirefreastr.nodes()))\n",
    "print(\"Number of Edges:\",len(merge_bioirefreastr.edges()))\n",
    "print(\"Degree of TP53:\",merge_bioirefreastr.degree('TP53'))\n",
    "print(\"Degree of PIK3CA:\",merge_bioirefreastr.degree('PIK3CA'))\n",
    "print(\"Degree of CDH1:\",merge_bioirefreastr.degree('CDH1'))\n",
    "write_nx_to_file(merge_bioirefreastr,\"./networks/merge_bioirefreastr_index_gene\",\"./networks/merge_bioirefreastr_edge_list\",\"./networks/merge_bioirefreastr_edge_list.sif\",\"./networks/merge_bioirefreastr_edge_list.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 3: generate p-values\n",
    "## (a) p-values for CORUM\n",
    "## (a).1 sampling target subnetworks\n",
    "## (a).2 sampling p-values 110 times, a = 0.01 to 0.11, \n",
    "## (a).3 transform p-values to q-score, locfdr, gene list, etc\n",
    "## (a).4 save all data\n",
    "## (a).5 send all data to CCR\n",
    "np.random.seed(seed=42)\n",
    "#load genes\n",
    "tmp = pd.read_csv('./networks/merge_bioirefreastr_index_gene',sep='\\t',header=None)\n",
    "total_gene = list(tmp[1])\n",
    "tmp_bio = pd.read_csv('./networks/biogrid_index_gene',sep='\\t',header=None)\n",
    "bio_gene = list(tmp_bio[1])\n",
    "tmp_iref = pd.read_csv('./networks/irefindex18_index_gene',sep='\\t',header=None)\n",
    "iref_gene = list(tmp_iref[1])\n",
    "tmp_rea = pd.read_csv('./networks/reactome21_index_gene',sep='\\t',header=None)\n",
    "rea_gene = list(tmp_rea[1])\n",
    "tmp_str = pd.read_csv('./networks/string_index_gene',sep='\\t',header=None)\n",
    "str_gene = list(tmp_str[1])\n",
    "\n",
    "# CORUM\n",
    "base_path = '/Users/leyang/Dropbox/Projects/Subnetwork_survey'\n",
    "for i in range(5):\n",
    "    name = \"corum_\" + str(i)\n",
    "    path = os.path.join(base_path,'code','score',name)\n",
    "    path_p = os.path.join(base_path,'code','score',name,'p')\n",
    "    path_q = os.path.join(base_path,'code','score',name,'q')\n",
    "    path_z = os.path.join(base_path,'code','score',name,'z')\n",
    "    path_lfdr = os.path.join(base_path,'code','score',name,'lfdr')\n",
    "    path_list = os.path.join(base_path,'code','score',name,'list')\n",
    "    path_clustex = os.path.join(base_path,'code','score',name,'clustex')\n",
    "    path_diamond = os.path.join(base_path,'code','score',name,'diamond')\n",
    "    path_all = [path_p,path_q,path_z,path_lfdr,path_list,path_clustex,path_diamond]\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    for pa in path_all:\n",
    "        if not os.path.exists(pa):\n",
    "            os.makedirs(pa)\n",
    "    index = np.random.choice(len(corum_final),size=10,replace=False)\n",
    "    corum_target = [corum_final[x][3] for x in range(len(corum_final)) if x in index]\n",
    "    corum_target_save = [corum_final_save[x] for x in range(len(corum_final_save)) if x in index]\n",
    "    tmp_df = pd.DataFrame(corum_target_save,columns=['ID','Name','Size','Genes'])\n",
    "    tmp_df.to_csv(os.path.join(path,name+'_targets.txt'),sep='\\t',index=False) #save targets\n",
    "    target_genes = set([x for l in corum_target for x in l])\n",
    "    \n",
    "    target = [x for x in target_genes if x in total_gene]\n",
    "    non_target = [x for x in total_gene if x not in target]\n",
    "    a_list = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11]\n",
    "    for a in a_list:\n",
    "        for permu in range(1,11):\n",
    "            \n",
    "            target_p = beta.rvs(a=a, b=1, size=len(target))\n",
    "            p_dict = dict(zip(target,target_p))\n",
    "            non_target_p = uniform.rvs(size=len(non_target))\n",
    "            non_target_dict = dict(zip(non_target,non_target_p))\n",
    "            p_dict.update(non_target_dict)\n",
    "            \n",
    "            # BH method to p_adjusted, then -log10 to qvalues\n",
    "            p_vals = [p_dict[k] for k in p_dict.keys()]\n",
    "            tmp,p_adjusted,tmp,tmp = multipletests(p_vals,method='fdr_bh')\n",
    "            q_score = [-math.log10(p) if p!=0 else 200 for p in p_adjusted]\n",
    "            q_dict = dict(zip(p_dict.keys(),q_score))\n",
    "            \n",
    "            # calculate z-values\n",
    "            z_vals = [-norm.ppf(p) for p in p_vals]\n",
    "            z_dict = dict(zip(p_dict.keys(),z_vals))\n",
    "            \n",
    "            # calculate lfdr\n",
    "            \n",
    "            index_noninfs = [i for i in range(len(z_vals)) if z_vals[i] != -float('Inf') and z_vals[i] != float('Inf')]\n",
    "            results = locfdr([z_vals[i] for i in index_noninfs],bre=120,df=10,saveplot=True,saveroot='./locfdr_plot/'+name+'beta_'+str(a)+'_'+str(permu),showplot=False)\n",
    "            fdr_noninfs = results['fdr']\n",
    "            fdr_dict = dict(zip(index_noninfs,fdr_noninfs))\n",
    "            fdr = [1.0 for i in range(len(z_vals))]\n",
    "            for i in range(len(z_vals)):\n",
    "                if i in index_noninfs:\n",
    "                    fdr[i] = fdr_dict[i]\n",
    "                elif z_vals[i] == float('Inf'):\n",
    "                    fdr[i] = 0\n",
    "            for i in range(len(fdr)):\n",
    "                if z_vals[i] < 0:\n",
    "                    fdr[i] = 1.0\n",
    "                if (z_vals[i] > 0) and math.isnan(fdr[i]):\n",
    "                    fdr[i] = 0.0\n",
    "            lfdr_dict = dict(zip(p_dict.keys(),fdr))\n",
    "            \n",
    "            # gene list\n",
    "            gene_list = [k for k,v in lfdr_dict.items() if v <= 0.1]\n",
    "            gene_list_index_bio = [bio_gene.index(g) + 1 for g in gene_list if g in bio_gene]\n",
    "            gene_list_index_iref = [iref_gene.index(g) + 1 for g in gene_list if g in iref_gene]\n",
    "            gene_list_index_rea = [rea_gene.index(g) + 1 for g in gene_list if g in rea_gene]\n",
    "            gene_list_index_str = [str_gene.index(g) + 1 for g in gene_list if g in str_gene]\n",
    "            \n",
    "            \n",
    "            # save all scores to files\n",
    "            p_df = pd.DataFrame(sorted(p_dict.items()),columns=['Gene','p'])\n",
    "            p_df.to_csv(os.path.join(path_p,name+'_p_beta_'+str(a)+'_'+str(permu)+'.txt'),header=None,sep='\\t',index=False)\n",
    "            q_df = pd.DataFrame(sorted(q_dict.items()),columns=['Gene','q'])\n",
    "            q_df.to_csv(os.path.join(path_q,name+'_q_beta_'+str(a)+'_'+str(permu)+'.txt'),header=None,sep='\\t',index=False)\n",
    "            z_df = pd.DataFrame(sorted(z_dict.items()),columns=['Gene','z'])\n",
    "            z_df.to_csv(os.path.join(path_z,name+'_z_beta_'+str(a)+'_'+str(permu)+'.txt'),header=None,sep='\\t',index=False)\n",
    "            lfdr_df = pd.DataFrame(sorted(lfdr_dict.items()),columns=['Gene','lfdr'])\n",
    "            lfdr_df.to_csv(os.path.join(path_lfdr,name+'_lfdr_beta_'+str(a)+'_'+str(permu)+'.txt'),header=None,sep='\\t',index=False)\n",
    "            \n",
    "            # gene list\n",
    "            with open(os.path.join(path_list,name+'_list_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "                    \n",
    "            # clustex biogrid\n",
    "            with open(os.path.join(path_clustex,name+'_clustex_biogrid_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                file.write(\"Gene\\n\")\n",
    "                for g in gene_list_index_bio:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # clustex irefindex\n",
    "            with open(os.path.join(path_clustex,name+'_clustex_irefindex18_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                file.write(\"Gene\\n\")\n",
    "                for g in gene_list_index_iref:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # clustex reactome21\n",
    "            with open(os.path.join(path_clustex,name+'_clustex_reactome21_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                file.write(\"Gene\\n\")\n",
    "                for g in gene_list_index_rea:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # clustex string\n",
    "            with open(os.path.join(path_clustex,name+'_clustex_string_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                file.write(\"Gene\\n\")\n",
    "                for g in gene_list_index_str:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "                    \n",
    "            # diamond biogrid\n",
    "            with open(os.path.join(path_diamond,name+'_diamond_biogrid_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list_index_bio:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # diamond irefindex\n",
    "            with open(os.path.join(path_diamond,name+'_diamond_irefindex18_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list_index_iref:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # diamond reactome21\n",
    "            with open(os.path.join(path_diamond,name+'_diamond_reactome21_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list_index_rea:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # diamond string\n",
    "            with open(os.path.join(path_diamond,name+'_diamond_string_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list_index_str:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "                    \n",
    "## Reactome\n",
    "for i in range(5):\n",
    "    name = \"reactome_\" + str(i)\n",
    "    path = os.path.join(base_path,'code','score',name)\n",
    "    path_p = os.path.join(base_path,'code','score',name,'p')\n",
    "    path_q = os.path.join(base_path,'code','score',name,'q')\n",
    "    path_z = os.path.join(base_path,'code','score',name,'z')\n",
    "    path_lfdr = os.path.join(base_path,'code','score',name,'lfdr')\n",
    "    path_list = os.path.join(base_path,'code','score',name,'list')\n",
    "    path_clustex = os.path.join(base_path,'code','score',name,'clustex')\n",
    "    path_diamond = os.path.join(base_path,'code','score',name,'diamond')\n",
    "    path_all = [path_p,path_q,path_z,path_lfdr,path_list,path_clustex,path_diamond]\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    for pa in path_all:\n",
    "        if not os.path.exists(pa):\n",
    "            os.makedirs(pa)\n",
    "    index = np.random.choice(len(reactome_final),size=10,replace=False)\n",
    "    reactome_target = [reactome_final[x][2] for x in range(len(reactome_final)) if x in index]\n",
    "    reactome_target_save = [reactome_final_save[x] for x in range(len(reactome_final_save)) if x in index]\n",
    "    tmp_df = pd.DataFrame(reactome_target_save,columns=['ID','Name','Size','Genes'])\n",
    "    tmp_df.to_csv(os.path.join(path,name+'_targets.txt'),sep='\\t',index=False) #save targets\n",
    "    target_genes = set([x for l in reactome_target for x in l])\n",
    "    \n",
    "    target = [x for x in target_genes if x in total_gene]\n",
    "    non_target = [x for x in total_gene if x not in target]\n",
    "    a_list = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11]\n",
    "    for a in a_list:\n",
    "        for permu in range(1,11):\n",
    "            \n",
    "            target_p = beta.rvs(a=a, b=1, size=len(target))\n",
    "            p_dict = dict(zip(target,target_p))\n",
    "            non_target_p = uniform.rvs(size=len(non_target))\n",
    "            non_target_dict = dict(zip(non_target,non_target_p))\n",
    "            p_dict.update(non_target_dict)\n",
    "            \n",
    "            # BH method to p_adjusted, then -log10 to qvalues\n",
    "            p_vals = [p_dict[k] for k in p_dict.keys()]\n",
    "            tmp,p_adjusted,tmp,tmp = multipletests(p_vals,method='fdr_bh')\n",
    "            q_score = [-math.log10(p) if p!=0 else 200 for p in p_adjusted]\n",
    "            q_dict = dict(zip(p_dict.keys(),q_score))\n",
    "            \n",
    "            # calculate z-values\n",
    "            z_vals = [-norm.ppf(p) for p in p_vals]\n",
    "            z_dict = dict(zip(p_dict.keys(),z_vals))\n",
    "            \n",
    "            # calculate lfdr\n",
    "            \n",
    "            index_noninfs = [i for i in range(len(z_vals)) if z_vals[i] != -float('Inf') and z_vals[i] != float('Inf')]\n",
    "            results = locfdr([z_vals[i] for i in index_noninfs],bre=120,df=10,saveplot=True,saveroot='./locfdr_plot/'+name+'beta_'+str(a)+'_'+str(permu),showplot=False)\n",
    "            fdr_noninfs = results['fdr']\n",
    "            fdr_dict = dict(zip(index_noninfs,fdr_noninfs))\n",
    "            fdr = [1.0 for i in range(len(z_vals))]\n",
    "            for i in range(len(z_vals)):\n",
    "                if i in index_noninfs:\n",
    "                    fdr[i] = fdr_dict[i]\n",
    "                elif z_vals[i] == float('Inf'):\n",
    "                    fdr[i] = 0\n",
    "            for i in range(len(fdr)):\n",
    "                if z_vals[i] < 0:\n",
    "                    fdr[i] = 1.0\n",
    "                if (z_vals[i] > 0) and math.isnan(fdr[i]):\n",
    "                    fdr[i] = 0.0\n",
    "            lfdr_dict = dict(zip(p_dict.keys(),fdr))\n",
    "            \n",
    "            # gene list\n",
    "            gene_list = [k for k,v in lfdr_dict.items() if v <= 0.1]\n",
    "            gene_list_index_bio = [bio_gene.index(g) + 1 for g in gene_list if g in bio_gene]\n",
    "            gene_list_index_iref = [iref_gene.index(g) + 1 for g in gene_list if g in iref_gene]\n",
    "            gene_list_index_rea = [rea_gene.index(g) + 1 for g in gene_list if g in rea_gene]\n",
    "            gene_list_index_str = [str_gene.index(g) + 1 for g in gene_list if g in str_gene]\n",
    "            \n",
    "            \n",
    "            # save all scores to files\n",
    "            p_df = pd.DataFrame(sorted(p_dict.items()),columns=['Gene','p'])\n",
    "            p_df.to_csv(os.path.join(path_p,name+'_p_beta_'+str(a)+'_'+str(permu)+'.txt'),header=None,sep='\\t',index=False)\n",
    "            q_df = pd.DataFrame(sorted(q_dict.items()),columns=['Gene','q'])\n",
    "            q_df.to_csv(os.path.join(path_q,name+'_q_beta_'+str(a)+'_'+str(permu)+'.txt'),header=None,sep='\\t',index=False)\n",
    "            z_df = pd.DataFrame(sorted(z_dict.items()),columns=['Gene','z'])\n",
    "            z_df.to_csv(os.path.join(path_z,name+'_z_beta_'+str(a)+'_'+str(permu)+'.txt'),header=None,sep='\\t',index=False)\n",
    "            lfdr_df = pd.DataFrame(sorted(lfdr_dict.items()),columns=['Gene','lfdr'])\n",
    "            lfdr_df.to_csv(os.path.join(path_lfdr,name+'_lfdr_beta_'+str(a)+'_'+str(permu)+'.txt'),header=None,sep='\\t',index=False)\n",
    "            \n",
    "            # gene list\n",
    "            with open(os.path.join(path_list,name+'_list_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "                    \n",
    "            # clustex biogrid\n",
    "            with open(os.path.join(path_clustex,name+'_clustex_biogrid_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                file.write(\"Gene\\n\")\n",
    "                for g in gene_list_index_bio:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # clustex irefindex\n",
    "            with open(os.path.join(path_clustex,name+'_clustex_irefindex18_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                file.write(\"Gene\\n\")\n",
    "                for g in gene_list_index_iref:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # clustex reactome21\n",
    "            with open(os.path.join(path_clustex,name+'_clustex_reactome21_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                file.write(\"Gene\\n\")\n",
    "                for g in gene_list_index_rea:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # clustex string\n",
    "            with open(os.path.join(path_clustex,name+'_clustex_string_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                file.write(\"Gene\\n\")\n",
    "                for g in gene_list_index_str:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "                    \n",
    "            # diamond biogrid\n",
    "            with open(os.path.join(path_diamond,name+'_diamond_biogrid_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list_index_bio:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # diamond irefindex\n",
    "            with open(os.path.join(path_diamond,name+'_diamond_irefindex18_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list_index_iref:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # diamond reactome21\n",
    "            with open(os.path.join(path_diamond,name+'_diamond_reactome21_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list_index_rea:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "            # diamond string\n",
    "            with open(os.path.join(path_diamond,name+'_diamond_string_beta_'+str(a)+'_'+str(permu)+'.txt'),'w') as file:\n",
    "                for g in gene_list_index_str:\n",
    "                    file.write(\"%s\\n\" % g)\n",
    "         \n",
    "                    \n",
    "                    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## send networks and scores to CCR\n",
    "data_version_list = [\"corum_\"+str(i) for i in range(5)] +  [\"reactome_\"+str(i) for i in range(5)]\n",
    "beta = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11]\n",
    "permu = [1,2,3,4,5,6,7,8,9,10]\n",
    "network_list = [\"biogrid\",\"irefindex18\",\"reactome21\",\"string\"]\n",
    "\n",
    "## data\n",
    "fdrdata = [os.path.join(\"./\",\"score\",data_version,\"lfdr\",data_version+\"_lfdr_beta_\"+str(b)+\"_\"+str(p)+\".txt\") for b in beta for p in permu for data_version in data_version_list]\n",
    "pdata = [os.path.join(\"./\",\"score\",data_version,\"p\",data_version+\"_p_beta_\"+str(b)+\"_\"+str(p)+\".txt\") for b in beta for p in permu for data_version in data_version_list]\n",
    "qdata = [os.path.join(\"./\",\"score\",data_version,\"q\",data_version+\"_q_beta_\"+str(b)+\"_\"+str(p)+\".txt\") for b in beta for p in permu for data_version in data_version_list]\n",
    "zdata = [os.path.join(\"./\",\"score\",data_version,\"z\",data_version+\"_z_beta_\"+str(b)+\"_\"+str(p)+\".txt\") for b in beta for p in permu for data_version in data_version_list]\n",
    "genelistdata = [os.path.join(\"./\",\"score\",data_version,\"list\",data_version+\"_list_beta_\"+str(b)+\"_\"+str(p)+\".txt\") for b in beta for p in permu for data_version in data_version_list]\n",
    "clustexdata = [os.path.join(\"./\",\"score\",data_version,\"clustex\",data_version+\"_clustex_\"+network+\"_beta_\"+str(b)+\"_\"+str(p)+\".txt\") for b in beta for p in permu for data_version in data_version_list for network in network_list]\n",
    "diamonddata = [os.path.join(\"./\",\"score\",data_version,\"diamond\",data_version+\"_diamond_\"+network+\"_beta_\"+str(b)+\"_\"+str(p)+\".txt\") for b in beta for p in permu for data_version in data_version_list for network in network_list]\n",
    "## network\n",
    "network_index_gene = [os.path.join(\"./\",\"networks\",network+\"_index_gene\") for network in network_list]\n",
    "network_edge_list = [os.path.join(\"./\",\"networks\",network+\"_edge_list\") for network in network_list]\n",
    "network_edge_sif = [os.path.join(\"./\",\"networks\",network+\"_edge_list.sif\") for network in network_list]\n",
    "network_edge_tsv = [os.path.join(\"./\",\"networks\",network+\"_edge_list.tsv\") for network in network_list]\n",
    "network_clustex = [os.path.join(\"./\",\"networks\",network+\"_edge_list_clustex.txt\") for network in network_list]\n",
    "## ensure data exists\n",
    "for f in fdrdata + pdata + qdata + zdata + genelistdata + clustexdata + diamonddata + network_index_gene + network_edge_list + network_edge_sif + network_edge_tsv:\n",
    "    assert os.path.exists(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send to domino\n",
    "subprocess.run([\"scp\",*network_edge_sif,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/domino/network/\"])\n",
    "subprocess.run([\"scp\",*genelistdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/domino/data/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to diamond\n",
    "subprocess.run([\"scp\",*network_edge_list,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/diamond/network/\"])\n",
    "subprocess.run([\"scp\",*diamonddata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/diamond/data/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to must\n",
    "# network\n",
    "subprocess.run([\"scp\",*network_edge_tsv,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/must/network/\"])\n",
    "subprocess.run([\"scp\",*genelistdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/must/data/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62393010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to clustex\n",
    "# network\n",
    "subprocess.run([\"scp\",*network_clustex,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/clustex/network/\"])\n",
    "subprocess.run([\"scp\",*clustexdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/clustex/data/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c67d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to netcore\n",
    "# network\n",
    "subprocess.run([\"scp\",*network_edge_tsv,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/netcore/network/\"])\n",
    "subprocess.run([\"scp\",*genelistdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/netcore/data/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to robust\n",
    "# network\n",
    "subprocess.run([\"scp\",*network_edge_tsv,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/robust/network/\"])\n",
    "subprocess.run([\"scp\",*genelistdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/robust/data/\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50f5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to BioNet\n",
    "# network\n",
    "#subprocess.run([\"scp\",*network_index_gene,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/bionet/network/\"])\n",
    "#subprocess.run([\"scp\",*network_edge_list,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/bionet/network/\"])\n",
    "# score\n",
    "subprocess.run([\"scp\",*pdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/bionet/data/\"])\n",
    "# send to hotnet2\n",
    "subprocess.run([\"scp\",*qdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/hotnet2/survey/data/heats/\"])\n",
    "# send to netmix2\n",
    "subprocess.run([\"scp\",*pdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/netmix2/data/\"])\n",
    "#subprocess.run([\"scp\",*network_edge_tsv,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/netmix2/network/\"])\n",
    "# send to FDRnet\n",
    "subprocess.run([\"scp\",*fdrdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/fdrnet/data/\"])\n",
    "# network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b096c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04811d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad80ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subprocess.run([\"scp\",*network_index_gene,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/fdrnet/network/\"])\n",
    "subprocess.run([\"scp\",*network_edge_list,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/fdrnet/network/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52425d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to FDRnet\n",
    "subprocess.run([\"scp\",*fdrdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/data/\"])\n",
    "\n",
    "# send to hhotnet\n",
    "subprocess.run([\"scp\",*qdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/hhotnet/biogrid/Data/\"])\n",
    "\n",
    "# send to netmix2\n",
    "subprocess.run([\"scp\",*pdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/netmix2/data/\"])\n",
    "\n",
    "# send to domino\n",
    "subprocess.run([\"scp\",*genelistdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/domino/Data/\"])\n",
    "\n",
    "# send to BioNet\n",
    "subprocess.run([\"scp\",*pdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/bionet/data/\"])\n",
    "\n",
    "# send to clustex\n",
    "subprocess.run([\"scp\",*clustexdata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/clustex/data/\"])\n",
    "\n",
    "# send to diamond\n",
    "subprocess.run([\"scp\",*diamonddata,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey/diamond/data/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # send to regmod\n",
    "# subprocess.run([\"scp\",*data_mat,\"lyang25@vortex.ccr.buffalo.edu:/projects/academic/yijunsun/leyang/survey_others/regmod/data/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbb92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55360642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\";\n",
    "network_path = \"./network\"\n",
    "result_folder = \"./result\"\n",
    "network_list = [\"biogrid\",\"irefindex18\",\"reactome21\",\"string\"]\n",
    "network = network_list*1100#[net for net in network_list for i in range(1100)]\n",
    "data_version_list = [\"corum_\"+str(k) for k in range(5)] + [\"reactome_\"+str(k) for k in range(5)]\n",
    "beta = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11]\n",
    "permu = [1,2,3,4,5,6,7,8,9,10]\n",
    "score = [\"\".join((data_version,\"_diamond_\"+net+\"_beta_\",str(b),\"_\",str(p))) for b in beta for p in permu for data_version in data_version_list for net in network_list]\n",
    "#python run_netmix2.py -el data/edge_list.tsv -gs data/gene_scores.tsv -o results/example_output.tsv\n",
    "\n",
    "for n in range(4400):\n",
    "    assert network[n] in score[n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e78f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
